import pandas as pd
import numpy as np
from itertools import combinations
from collections import defaultdict
import datetime
import os
import json
import sys
import argparse
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload

# ==========================================
# è¨­å®šå€
# ==========================================
FILE_539 = 'lottery_hist.xlsx'
FILE_FANTASY = 'fantasy5_hist.xlsx'

OUTPUT_539 = 'best_strategies_539.csv'
OUTPUT_FANTASY = 'best_strategies_fantasy5.csv'

# è¦–çª—å®šç¾© (å°æ‡‰ Python weekday: 0=é€±ä¸€ ... 6=é€±æ—¥)
WINDOWS_MAPPING = {
    "é€±ä¸€~é€±ä¸‰": [0, 1, 2],
    "é€±äºŒ~é€±å››": [1, 2, 3],
    "é€±ä¸‰~é€±äº”": [2, 3, 4],
    "é€±å››~é€±å…­": [3, 4, 5]
}

# ==========================================
# æ ¸å¿ƒæ¼”ç®—æ³•
# ==========================================

def load_data(file_path, is_fantasy=False):
    """è®€å–è³‡æ–™ä¸¦è™•ç†æ™‚å€"""
    if not os.path.exists(file_path):
        return None
    
    try:
        df = pd.read_excel(file_path, engine='openpyxl')
    except:
        try:
            # å˜—è©¦è®€å– CSVï¼ˆå¯èƒ½æ˜¯ fantasy5_hist.xlsx - Sheet1.csvï¼‰
            csv_path = file_path.replace('.xlsx', '.csv')
            if not os.path.exists(csv_path):
                # å˜—è©¦å…¶ä»–å¯èƒ½çš„ CSV æª”å
                csv_path = file_path.replace('.xlsx', ' - Sheet1.csv')
            df = pd.read_csv(csv_path) # å‚™æ´
        except:
            return None

    # è™•ç†æ—¥æœŸæ¬„ä½ï¼šæ”¯æ´å¤šç¨®æ—¥æœŸæ ¼å¼ï¼ˆåŒ…å«æˆ–ä¸åŒ…å«æ™‚é–“ï¼‰
    try:
        # å…ˆå˜—è©¦è§£æç‚º datetimeï¼Œè®“ pandas è‡ªå‹•æ¨æ–·æ ¼å¼
        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], format='mixed', errors='coerce')
        # å¦‚æœè‡ªå‹•æ¨æ–·å¤±æ•—ï¼Œå˜—è©¦å¸¸è¦‹æ ¼å¼
        if df['æ—¥æœŸ'].isna().any():
            df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], format='%Y-%m-%d %H:%M:%S', errors='coerce')
        if df['æ—¥æœŸ'].isna().any():
            df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], format='%Y-%m-%d', errors='coerce')
    except:
        # æœ€å¾Œå˜—è©¦ä¸æŒ‡å®šæ ¼å¼ï¼Œè®“ pandas è‡ªå‹•è™•ç†
        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce')
    
    # ç§»é™¤ç„¡æ³•è§£æçš„æ—¥æœŸè¡Œ
    df = df.dropna(subset=['æ—¥æœŸ'])
    
    # æ™‚å€è™•ç†
    if is_fantasy:
        # å¤©å¤©æ¨‚: ç¾åœ‹æ™‚é–“ + 1å¤© = å°ç£ä¸‹æ³¨æ™‚é–“
        df['Analysis_Date'] = df['æ—¥æœŸ'] + pd.Timedelta(days=1)
    else:
        # 539: ä¸éœ€è¦è½‰æ›
        df['Analysis_Date'] = df['æ—¥æœŸ']

    return df

def get_data_by_week(df):
    """å°‡è³‡æ–™è½‰æ›ç‚º {(å¹´, é€±): {æ˜ŸæœŸå¹¾: {è™Ÿç¢¼é›†åˆ}}}"""
    data = defaultdict(lambda: defaultdict(set))
    for _, row in df.iterrows():
        dt = row['Analysis_Date']
        year, week, _ = dt.isocalendar()
        weekday = dt.weekday()
        
        # å˜—è©¦æŠ“å–è™Ÿç¢¼æ¬„ä½ (ç›¸å®¹ä¸åŒå‘½å)
        try:
            # å„ªå…ˆå˜—è©¦æ¨™æº–æ¬„ä½
            if 'è™Ÿç¢¼1' in df.columns:
                nums = {row[c] for c in ['è™Ÿç¢¼1', 'è™Ÿç¢¼2', 'è™Ÿç¢¼3', 'è™Ÿç¢¼4', 'è™Ÿç¢¼5']}
            else:
                # å‡è¨­çµæ§‹å›ºå®š
                cols = df.columns
                nums = {row[cols[2]], row[cols[3]], row[cols[4]], row[cols[5]], row[cols[6]]}
        except:
            continue
            
        data[(year, week)][weekday] = nums
    return data

def calculate_stats(data_by_week, weeks_list, mode="win_rate"):
    """
    é€šç”¨è¨ˆç®—æ ¸å¿ƒ
    mode="win_rate": è¨ˆç®—å‹ç‡ (çŸ­æœŸ/é•·æœŸ)
    mode="streak": è¨ˆç®—é€£èŠ (å¾æœ€æ–°é€±å¾€å›æ¨)
    """
    results = []
    all_combos = list(combinations(range(1, 40), 3)) # 1-39è™Ÿå–3å€‹
    
    # é è™•ç†æ¯é€±çš„è™Ÿç¢¼è¯é›† (é‡å°ä¸åŒè¦–çª—)
    # week_unions[window_name][week_key] = set(all numbers in that window)
    week_unions = defaultdict(dict)
    
    # å¦‚æœæ˜¯é€£èŠæ¨¡å¼ï¼Œå¿…é ˆç¢ºä¿é€±æ¬¡æ˜¯å€’åº (æœ€æ–° -> æœ€èˆŠ)
    target_weeks = sorted(weeks_list, reverse=True) if mode == "streak" else weeks_list
    
    for window_name, days in WINDOWS_MAPPING.items():
        for w in target_weeks:
            union_set = set()
            has_data = False
            for d in days:
                if d in data_by_week[w]:
                    union_set.update(data_by_week[w][d])
                    has_data = True
            if has_data:
                week_unions[window_name][w] = union_set

    # é–‹å§‹éæ­·æ‰€æœ‰çµ„åˆ (9139çµ„)
    for combo in all_combos:
        combo_set = set(combo)
        
        for window_name in WINDOWS_MAPPING.keys():
            valid_weeks = [w for w in target_weeks if w in week_unions[window_name]]
            if len(valid_weeks) < 4 and mode == "win_rate": continue
            
            if mode == "win_rate":
                wins = 0
                for w in valid_weeks:
                    # åˆ¤æ–·ä¸­ç: çµ„åˆ èˆ‡ ç•¶é€±é–‹çè™Ÿç¢¼ æœ‰äº¤é›†
                    if not combo_set.isdisjoint(week_unions[window_name][w]):
                        wins += 1
                
                rate = wins / len(valid_weeks)
                # é–€æª»éæ¿¾
                if rate >= 0.8: # å¯¬é¬†é–€æª»ï¼Œå¾ŒçºŒç¯©é¸ Top 2
                    results.append({
                        "Window": window_name,
                        "Combo": combo,
                        "Score": rate, # æ’åºç”¨
                        "Display": f"{rate:.1%} ({wins}/{len(valid_weeks)})"
                    })
            
            elif mode == "streak":
                streak = 0
                for w in valid_weeks:
                    if not combo_set.isdisjoint(week_unions[window_name][w]):
                        streak += 1
                    else:
                        break # ä¸­æ–·
                
                if streak >= 4: # è‡³å°‘é€£4é€±æ‰ç´€éŒ„
                    results.append({
                        "Window": window_name,
                        "Combo": combo,
                        "Score": streak,
                        "Display": f"{streak}é€±"
                    })

    return pd.DataFrame(results)

def select_best_strategies(df, threshold=0.0):
    """
    æŒ‘é¸é‚è¼¯:
    1. éæ¿¾åˆ†æ•¸ < threshold
    2. é¸ç¬¬ä¸€å (Score æœ€é«˜)
    3. é¸ç¬¬äºŒå (Score æ¬¡é«˜ï¼Œä¸” Window èˆ‡ç¬¬ä¸€åä¸åŒ)
    """
    if df.empty:
        return "ç„¡æ•¸æ“š", "ç„¡æ•¸æ“š"
        
    df = df[df['Score'] >= threshold].sort_values('Score', ascending=False)
    if df.empty:
        return "ç„¡æ•¸æ“š", "ç„¡æ•¸æ“š"
        
    # ç¬¬ä¸€å
    top1 = df.iloc[0]
    
    # ç¬¬äºŒå (äº’æ–¥è¦–çª—)
    top2 = None
    for _, row in df.iterrows():
        if row['Window'] != top1['Window']:
            top2 = row
            break
            
    def format_row(row):
        nums = ",".join(f"{x:02d}" for x in row['Combo'])
        return f"ã€{row['Window']}ã€‘{nums} [{row['Display']}]"

    res1 = format_row(top1)
    res2 = format_row(top2) if top2 is not None else "ç„¡äº’æ–¥æ™‚æ®µæ•¸æ“š"
    
    return res1, res2

# ==========================================
# Google Drive ä¸Šå‚³
# ==========================================
def upload_to_drive(local_file, folder_id, creds_json):
    """ä¸Šå‚³æ–‡ä»¶åˆ° Google Driveï¼Œå¦‚æœä¸å­˜åœ¨å‰‡å‰µå»º"""
    if not os.path.exists(local_file):
        print(f"âŒ æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨: {local_file}")
        return False
    
    if not folder_id:
        print(f"âš ï¸ æœªè¨­ç½® GOOGLE_DRIVE_FOLDER_ID")
        return False
    
    if not creds_json:
        print(f"âš ï¸ æœªè¨­ç½® GOOGLE_CREDENTIALS")
        return False

    try:
        # è§£æèªè­‰è³‡è¨Š
        if isinstance(creds_json, str):
            creds_dict = json.loads(creds_json)
        else:
            creds_dict = creds_json
        
        creds = service_account.Credentials.from_service_account_info(
            creds_dict, scopes=['https://www.googleapis.com/auth/drive']
        )
        service = build('drive', 'v3', credentials=creds)
        file_name = os.path.basename(local_file)

        # æœå°‹é›²ç«¯æ˜¯å¦å·²å­˜åœ¨è©²æ–‡ä»¶
        query = f"name = '{file_name}' and '{folder_id}' in parents and trashed = false"
        results = service.files().list(q=query, fields="files(id, name)").execute()
        files = results.get('files', [])

        media = MediaFileUpload(local_file, mimetype='text/csv')

        if not files:
            # é›²ç«¯ä¸å­˜åœ¨ï¼Œå‰µå»ºæ–°æ–‡ä»¶
            file_metadata = {
                'name': file_name,
                'parents': [folder_id]
            }
            created_file = service.files().create(
                body=file_metadata,
                media_body=media,
                fields='id,name,webViewLink'
            ).execute()
            print(f"âœ… [Drive] æ–°å¢æ–‡ä»¶: {file_name}")
            print(f"   ğŸ“ æ–‡ä»¶ ID: {created_file.get('id')}")
            print(f"   ğŸ”— æª¢è¦–é€£çµ: {created_file.get('webViewLink', 'N/A')}")
        else:
            # é›²ç«¯å·²å­˜åœ¨ï¼Œæ›´æ–°æ–‡ä»¶
            file_id = files[0]['id']
            service.files().update(
                fileId=file_id,
                media_body=media
            ).execute()
            print(f"âœ… [Drive] æ›´æ–°æ–‡ä»¶: {file_name} (ID: {file_id})")
        
        return True

    except json.JSONDecodeError as e:
        print(f"âŒ [Drive] èªè­‰è³‡è¨Šæ ¼å¼éŒ¯èª¤: {e}")
        return False
    except Exception as e:
        print(f"âŒ [Drive] ä¸Šå‚³å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

# ==========================================
# ä¸»æµç¨‹
# ==========================================
def process_single(name, input_file, output_file, is_fantasy, folder_id, creds):
    """è™•ç†å–®ä¸€å½©çƒçš„åˆ†æ"""
    print(f"\nâš¡ åˆ†æ {name} (è½‰æ›æ™‚å€: {is_fantasy})...")
    df = load_data(input_file, is_fantasy)
    if df is None:
        print(f"âŒ æ‰¾ä¸åˆ° {input_file}ï¼Œè·³é")
        return False
        
    # æ—¥æœŸç¯©é¸
    max_date = df['Analysis_Date'].max()
    cutoff_8wk = max_date - pd.Timedelta(weeks=8)
    cutoff_1yr = max_date - pd.Timedelta(weeks=52)
    
    data_by_week = get_data_by_week(df)
    all_weeks = sorted(data_by_week.keys())
    
    weeks_8wk = [w for w in all_weeks if w in get_data_by_week(df[df['Analysis_Date'] >= cutoff_8wk])][1:] # ç•¥éè³‡æ–™ä¸å…¨çš„ç•¶é€±
    weeks_1yr = [w for w in all_weeks if w in get_data_by_week(df[df['Analysis_Date'] >= cutoff_1yr])]
    
    # 1. çŸ­æœŸ (8é€±)
    print("   -> è¨ˆç®—çŸ­æœŸå‹ç‡...")
    df_short = calculate_stats(data_by_week, weeks_8wk, mode="win_rate")
    
    # 2. é•·æœŸ (1å¹´)
    print("   -> è¨ˆç®—é•·æœŸå‹ç‡...")
    df_long = calculate_stats(data_by_week, weeks_1yr, mode="win_rate")
    
    # 3. é€£èŠ
    print("   -> è¨ˆç®—é€£èŠéœ¸ä¸»...")
    df_streak = calculate_stats(data_by_week, all_weeks, mode="streak")
    
    # å½™æ•´
    report = []
    
    # çŸ­æœŸ (é–€æª» 85%)
    s1, s2 = select_best_strategies(df_short, threshold=0.85)
    report.append({"ç­–ç•¥ç¶­åº¦": "çŸ­æœŸçˆ†ç™¼ (è¿‘8é€±)", "ç¬¬ä¸€çµ„": s1, "ç¬¬äºŒçµ„": s2})
    
    # é•·æœŸ (é–€æª» 90%ï¼Œä½æ–¼é¡¯ç¤ºç„¡æ•¸æ“š)
    l1, l2 = select_best_strategies(df_long, threshold=0.90)
    report.append({"ç­–ç•¥ç¶­åº¦": "é•·æœŸç©©å¥ (è¿‘1å¹´)", "ç¬¬ä¸€çµ„": l1, "ç¬¬äºŒçµ„": l2})
    
    # é€£èŠ (è‡³å°‘é€£5é€±)
    st1, st2 = select_best_strategies(df_streak, threshold=5)
    report.append({"ç­–ç•¥ç¶­åº¦": "é€£èŠéœ¸ä¸» (é€£å‹ä¸­)", "ç¬¬ä¸€çµ„": st1, "ç¬¬äºŒçµ„": st2})
    
    # è¼¸å‡º CSV (ç¢ºä¿ä¸€å®šæœƒå‰µå»ºæ–‡ä»¶)
    res_df = pd.DataFrame(report)
    res_df.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"ğŸ“„ å·²å»ºç«‹: {output_file}")
    
    # é©—è­‰æ–‡ä»¶ç¢ºå¯¦å­˜åœ¨
    if not os.path.exists(output_file):
        print(f"âŒ è­¦å‘Š: {output_file} å‰µå»ºå¤±æ•—")
        return False
    
    # ä¸Šå‚³åˆ° Google Drive
    if folder_id and creds:
        try:
            upload_to_drive(output_file, folder_id, creds)
            print(f"âœ… {output_file} å·²ä¸Šå‚³åˆ° Google Drive")
        except Exception as e:
            print(f"âš ï¸ ä¸Šå‚³ {output_file} åˆ° Google Drive æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            print(f"   æœ¬åœ°æ–‡ä»¶å·²å‰µå»º: {output_file}")
    else:
        print(f"âš ï¸ æœªè¨­ç½® Google Drive ç’°å¢ƒè®Šæ•¸ï¼Œè·³éä¸Šå‚³")
        print(f"   éœ€è¦è¨­ç½®: GOOGLE_DRIVE_FOLDER_ID å’Œ GOOGLE_CREDENTIALS")
    
    return True

def process_all():
    """è™•ç†æ‰€æœ‰å½©çƒçš„åˆ†æï¼ˆé è¨­è¡Œç‚ºï¼‰"""
    folder_id = os.environ.get('GOOGLE_DRIVE_FOLDER_ID')
    creds = os.environ.get('GOOGLE_CREDENTIALS')

    tasks = [
        ("539", FILE_539, OUTPUT_539, False),
        ("å¤©å¤©æ¨‚", FILE_FANTASY, OUTPUT_FANTASY, True)
    ]

    for name, input_file, output_file, is_fantasy in tasks:
        process_single(name, input_file, output_file, is_fantasy, folder_id, creds)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='åˆ†æå½©çƒç­–ç•¥')
    parser.add_argument('--type', type=str, choices=['539', 'fantasy5', 'all'], 
                       default='all', help='æŒ‡å®šè¦åˆ†æçš„å½©çƒé¡å‹: 539, fantasy5, æˆ– all (é è¨­)')
    
    args = parser.parse_args()
    
    folder_id = os.environ.get('GOOGLE_DRIVE_FOLDER_ID')
    creds = os.environ.get('GOOGLE_CREDENTIALS')
    
    if args.type == '539':
        print("ğŸ¯ åƒ…åˆ†æ 539...")
        process_single("539", FILE_539, OUTPUT_539, False, folder_id, creds)
    elif args.type == 'fantasy5':
        print("ğŸ¯ åƒ…åˆ†æå¤©å¤©æ¨‚...")
        process_single("å¤©å¤©æ¨‚", FILE_FANTASY, OUTPUT_FANTASY, True, folder_id, creds)
    else:
        print("ğŸ¯ åˆ†ææ‰€æœ‰å½©çƒ...")
        process_all()