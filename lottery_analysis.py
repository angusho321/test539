# --------------------------------------------------------
#  lottery_analysis.py (xlsx ç‰ˆ)
# --------------------------------------------------------

import pandas as pd
import numpy as np
import random
from pathlib import Path
from datetime import datetime
# å°å…¥é©—è­‰åŠŸèƒ½ï¼ˆå¦‚æœæª”æ¡ˆå­˜åœ¨çš„è©±ï¼‰
try:
    from verify_predictions import auto_verify_on_startup
    VERIFICATION_AVAILABLE = True
except ImportError:
    VERIFICATION_AVAILABLE = False

# --------------------------------------------------------
# 1. è®€å–è³‡æ–™ --------------------------------------------------
def load_lottery_excel(excel_path: str):
    """
    è®€å…¥ .xlsx é–‹çç´€éŒ„ã€‚
    æ¬„ä½å‡è¨­ç‚ºï¼š
        æ—¥æœŸ, æ˜ŸæœŸ, è™Ÿç¢¼1, è™Ÿç¢¼2, è™Ÿç¢¼3, è™Ÿç¢¼4, è™Ÿç¢¼5
    """
    df = pd.read_excel(excel_path, engine='openpyxl')
    # è‹¥ä½ ä½¿ç”¨çš„æ˜¯èˆŠç‰ˆ pandasï¼Œengine åƒæ•¸å¯å»æ‰
    return df

# --------------------------------------------------------
# 2. è¨ˆç®—é »ç‡èˆ‡ç†±å†·è™Ÿ --------------------------------------------
def compute_num_frequency(df: pd.DataFrame):
    """å›å‚³æ¯å€‹è™Ÿç¢¼çš„é »æ¬¡ï¼ˆSeriesï¼‰"""
    nums = df[['è™Ÿç¢¼1','è™Ÿç¢¼2','è™Ÿç¢¼3','è™Ÿç¢¼4','è™Ÿç¢¼5']].values.ravel()
    freq = pd.Series(nums).value_counts().sort_index()
    freq.index.name = 'è™Ÿç¢¼'
    freq.name = 'é »æ¬¡'
    return freq

def get_hot_cold_numbers(freq: pd.Series, top_n=6, bottom_n=6):
    hot_numbers = freq.nlargest(top_n).index.tolist()
    cold_numbers = freq.nsmallest(bottom_n).index.tolist()
    return hot_numbers, cold_numbers

# --------------------------------------------------------
# 3. å¯è¦–åŒ–åŠŸèƒ½å·²ç§»é™¤ (åŸ plot_frequency å‡½æ•¸)

# --------------------------------------------------------
# 4. æ¨¡å¼åˆ†æåŠŸèƒ½ -------------------------------------------
def analyze_single_draw_patterns(numbers_row):
    """åˆ†æå–®æ¬¡é–‹ççš„æ¨¡å¼"""
    nums = sorted(numbers_row)
    
    # è¨ˆç®—0é ­è™Ÿç¢¼æ•¸é‡ï¼ˆ1-9ï¼‰
    zero_head_count = sum(1 for n in nums if n <= 9)
    
    # è¨ˆç®—1é ­è™Ÿç¢¼æ•¸é‡ï¼ˆ10-19ï¼‰
    one_head_count = sum(1 for n in nums if 10 <= n <= 19)
    
    # è¨ˆç®—2é ­è™Ÿç¢¼æ•¸é‡ï¼ˆ20-29ï¼‰
    two_head_count = sum(1 for n in nums if 20 <= n <= 29)
    
    # è¨ˆç®—3é ­è™Ÿç¢¼æ•¸é‡ï¼ˆ30-39ï¼‰
    three_head_count = sum(1 for n in nums if 30 <= n <= 39)
    
    # è¨ˆç®—é€£è™Ÿæƒ…æ³
    consecutive_count = 0
    max_consecutive = 1
    current_consecutive = 1
    has_consecutive = False  # æ˜¯å¦æœ‰é€£è™Ÿ
    
    for i in range(1, len(nums)):
        if nums[i] == nums[i-1] + 1:
            current_consecutive += 1
            has_consecutive = True
        else:
            max_consecutive = max(max_consecutive, current_consecutive)
            current_consecutive = 1
    max_consecutive = max(max_consecutive, current_consecutive)
    
    # è¨ˆç®—å¥‡å¶æ¯”ä¾‹
    odd_count = sum(1 for n in nums if n % 2 == 1)
    even_count = 5 - odd_count
    
    # è¨ˆç®—å¤§å°è™Ÿæ¯”ä¾‹ï¼ˆ1-19ç‚ºå°è™Ÿï¼Œ20-39ç‚ºå¤§è™Ÿï¼‰
    small_count = sum(1 for n in nums if n <= 19)
    large_count = 5 - small_count
    
    # è¨ˆç®—è·¨åº¦ï¼ˆæœ€å¤§è™Ÿ-æœ€å°è™Ÿï¼‰
    span = nums[-1] - nums[0]
    
    return {
        'zero_head_count': zero_head_count,
        'one_head_count': one_head_count,
        'two_head_count': two_head_count,
        'three_head_count': three_head_count,
        'max_consecutive': max_consecutive,
        'has_consecutive': has_consecutive,
        'odd_count': odd_count,
        'even_count': even_count,
        'small_count': small_count,
        'large_count': large_count,
        'span': span
    }

def pattern_statistics(df: pd.DataFrame):
    """åˆ†ææ­·å²é–‹çæ¨¡å¼çµ±è¨ˆ"""
    patterns = []
    
    for _, row in df.iterrows():
        numbers = [row['è™Ÿç¢¼1'], row['è™Ÿç¢¼2'], row['è™Ÿç¢¼3'], row['è™Ÿç¢¼4'], row['è™Ÿç¢¼5']]
        pattern = analyze_single_draw_patterns(numbers)
        patterns.append(pattern)
    
    patterns_df = pd.DataFrame(patterns)
    
    print("\n=== æ­·å²é–‹çæ¨¡å¼çµ±è¨ˆ ===")
    
    # 0é ­è™Ÿç¢¼çµ±è¨ˆ
    print(f"\n0é ­è™Ÿç¢¼(1-9)æ•¸é‡åˆ†å¸ƒ:")
    zero_head_dist = patterns_df['zero_head_count'].value_counts().sort_index()
    for count, freq in zero_head_dist.items():
        percentage = freq / len(patterns_df) * 100
        print(f"  {count}å€‹: {freq}æ¬¡ ({percentage:.1f}%)")
    
    # 1é ­è™Ÿç¢¼çµ±è¨ˆ
    print(f"\n1é ­è™Ÿç¢¼(10-19)æ•¸é‡åˆ†å¸ƒ:")
    one_head_dist = patterns_df['one_head_count'].value_counts().sort_index()
    for count, freq in one_head_dist.items():
        percentage = freq / len(patterns_df) * 100
        print(f"  {count}å€‹: {freq}æ¬¡ ({percentage:.1f}%)")
    
    # 2é ­è™Ÿç¢¼çµ±è¨ˆ
    print(f"\n2é ­è™Ÿç¢¼(20-29)æ•¸é‡åˆ†å¸ƒ:")
    two_head_dist = patterns_df['two_head_count'].value_counts().sort_index()
    for count, freq in two_head_dist.items():
        percentage = freq / len(patterns_df) * 100
        print(f"  {count}å€‹: {freq}æ¬¡ ({percentage:.1f}%)")
    
    # 3é ­è™Ÿç¢¼çµ±è¨ˆ
    print(f"\n3é ­è™Ÿç¢¼(30-39)æ•¸é‡åˆ†å¸ƒ:")
    three_head_dist = patterns_df['three_head_count'].value_counts().sort_index()
    for count, freq in three_head_dist.items():
        percentage = freq / len(patterns_df) * 100
        print(f"  {count}å€‹: {freq}æ¬¡ ({percentage:.1f}%)")
    
    # é€£è™Ÿçµ±è¨ˆ
    print(f"\næœ€å¤§é€£è™Ÿæ•¸é‡åˆ†å¸ƒ:")
    consecutive_dist = patterns_df['max_consecutive'].value_counts().sort_index()
    for count, freq in consecutive_dist.items():
        percentage = freq / len(patterns_df) * 100
        print(f"  {count}å€‹: {freq}æ¬¡ ({percentage:.1f}%)")
    
    # é€£è™Ÿå‡ºç¾æƒ…æ³çµ±è¨ˆ
    has_consecutive_count = patterns_df['has_consecutive'].sum()
    no_consecutive_count = len(patterns_df) - has_consecutive_count
    print(f"\né€£è™Ÿå‡ºç¾æƒ…æ³:")
    print(f"  æœ‰å‡ºç¾é€£è™Ÿ: {has_consecutive_count}æœŸ ({has_consecutive_count/len(patterns_df)*100:.1f}%)")
    print(f"  æ²’æœ‰é€£è™Ÿ: {no_consecutive_count}æœŸ ({no_consecutive_count/len(patterns_df)*100:.1f}%)")
    
    # å¥‡æ•¸æ¯”ä¾‹çµ±è¨ˆ
    print(f"\nå¥‡æ•¸æ•¸é‡åˆ†å¸ƒ:")
    odd_dist = patterns_df['odd_count'].value_counts().sort_index()
    for count, freq in odd_dist.items():
        percentage = freq / len(patterns_df) * 100
        print(f"  {count}å€‹å¥‡æ•¸: {freq}æ¬¡ ({percentage:.1f}%)")
    
    # é›™æ•¸æ¯”ä¾‹çµ±è¨ˆ
    print(f"\né›™æ•¸æ•¸é‡åˆ†å¸ƒ:")
    even_dist = patterns_df['even_count'].value_counts().sort_index()
    for count, freq in even_dist.items():
        percentage = freq / len(patterns_df) * 100
        print(f"  {count}å€‹é›™æ•¸: {freq}æ¬¡ ({percentage:.1f}%)")
    
    # å¤§å°è™Ÿæ¯”ä¾‹çµ±è¨ˆ
    print(f"\nå°è™Ÿ(1-19)æ•¸é‡åˆ†å¸ƒ:")
    small_dist = patterns_df['small_count'].value_counts().sort_index()
    for count, freq in small_dist.items():
        percentage = freq / len(patterns_df) * 100
        print(f"  {count}å€‹: {freq}æ¬¡ ({percentage:.1f}%)")
    
    # è·¨åº¦çµ±è¨ˆ
    print(f"\nè™Ÿç¢¼è·¨åº¦çµ±è¨ˆ:")
    print(f"  å¹³å‡è·¨åº¦: {patterns_df['span'].mean():.1f}")
    print(f"  æœ€å°è·¨åº¦: {patterns_df['span'].min()}")
    print(f"  æœ€å¤§è·¨åº¦: {patterns_df['span'].max()}")
    
    # ç¶œåˆçµ±è¨ˆæ‘˜è¦
    print(f"\nğŸ“ˆ çµ±è¨ˆæ‘˜è¦ (å…±{len(patterns_df)}æœŸ):")
    print(f"  â€¢ 0é ­è™Ÿç¢¼æœ€å¸¸å‡ºç¾: {zero_head_dist.idxmax()}å€‹ ({zero_head_dist.max()}æ¬¡)")
    print(f"  â€¢ 1é ­è™Ÿç¢¼æœ€å¸¸å‡ºç¾: {one_head_dist.idxmax()}å€‹ ({one_head_dist.max()}æ¬¡)")
    print(f"  â€¢ 2é ­è™Ÿç¢¼æœ€å¸¸å‡ºç¾: {two_head_dist.idxmax()}å€‹ ({two_head_dist.max()}æ¬¡)")
    print(f"  â€¢ 3é ­è™Ÿç¢¼æœ€å¸¸å‡ºç¾: {three_head_dist.idxmax()}å€‹ ({three_head_dist.max()}æ¬¡)")
    print(f"  â€¢ å¥‡æ•¸æœ€å¸¸å‡ºç¾: {odd_dist.idxmax()}å€‹ ({odd_dist.max()}æ¬¡)")
    print(f"  â€¢ é›™æ•¸æœ€å¸¸å‡ºç¾: {even_dist.idxmax()}å€‹ ({even_dist.max()}æ¬¡)")
    print(f"  â€¢ é€£è™Ÿè¶…é2å€‹çš„æ©Ÿç‡: {(consecutive_dist[consecutive_dist.index > 2].sum() / len(patterns_df) * 100):.1f}%")
    print(f"  â€¢ æœ‰é€£è™Ÿå‡ºç¾çš„æ©Ÿç‡: {(has_consecutive_count / len(patterns_df) * 100):.1f}%")
    print(f"  â€¢ å®Œå…¨æ²’æœ‰é€£è™Ÿçš„æ©Ÿç‡: {(no_consecutive_count / len(patterns_df) * 100):.1f}%")
    
    return patterns_df

def is_pattern_reasonable(numbers, historical_stats=None):
    """æª¢æŸ¥è™Ÿç¢¼çµ„åˆæ˜¯å¦ç¬¦åˆæ­·å²æ¨¡å¼"""
    pattern = analyze_single_draw_patterns(numbers)
    
    # å¦‚æœæœ‰æ­·å²çµ±è¨ˆæ•¸æ“šï¼Œä½¿ç”¨å‹•æ…‹æ¨™æº–ï¼›å¦å‰‡ä½¿ç”¨å›ºå®šæ¨™æº–
    if historical_stats is not None:
        # åŸºæ–¼æ­·å²çµ±è¨ˆçš„å‹•æ…‹é©—è­‰
        zero_head_common = historical_stats['zero_head_most_common']
        one_head_common = historical_stats['one_head_most_common']
        two_head_common = historical_stats['two_head_most_common']
        three_head_common = historical_stats['three_head_most_common']
        consecutive_prob = historical_stats['consecutive_prob']
        
        # å…è¨±å¸¸è¦‹çš„æ¨¡å¼çµ„åˆ
        if pattern['zero_head_count'] > zero_head_common + 1:
            return False
        if pattern['one_head_count'] > one_head_common + 1:
            return False
        if pattern['two_head_count'] > two_head_common + 1:
            return False
        if pattern['three_head_count'] > three_head_common + 1:
            return False
        if pattern['max_consecutive'] > 3:  # é€£è™Ÿä»ä¿æŒæœ€å¤§3å€‹çš„é™åˆ¶
            return False
    else:
        # å›ºå®šæ¨™æº–ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
        if pattern['zero_head_count'] > 3:
            return False
        if pattern['max_consecutive'] > 3:
            return False
    
    # è·¨åº¦æª¢æŸ¥
    if pattern['span'] < 10 or pattern['span'] > 35:
        return False
    
    return True

def is_trend_reasonable(numbers, historical_stats):
    """åŸºæ–¼è¿‘æœŸè¶¨å‹¢æª¢æŸ¥è™Ÿç¢¼çµ„åˆæ˜¯å¦åˆç†"""
    if not historical_stats.get('trend_detected', False):
        # æ²’æœ‰è¶¨å‹¢ï¼Œä½¿ç”¨æ¨™æº–é©—è­‰
        return is_pattern_reasonable(numbers, historical_stats)
    
    pattern = analyze_single_draw_patterns(numbers)
    trends = historical_stats.get('trends', {})
    recent_data = historical_stats.get('recent_vs_historical', {})
    
    if not recent_data:
        return is_pattern_reasonable(numbers, historical_stats)
    
    recent_avg = recent_data['recent']
    
    # æ ¹æ“šè¶¨å‹¢èª¿æ•´é©—è­‰æ¨™æº–
    # å¦‚æœè¿‘æœŸæŸå€‹ç‰¹å¾µå¢åŠ ï¼Œå‰‡æ›´å®¹æ˜“æ¥å—è¼ƒé«˜çš„å€¼
    
    # 0é ­è™Ÿç¢¼è¶¨å‹¢èª¿æ•´
    if 'zero_head_avg_trend' in trends:
        if trends['zero_head_avg_trend'] == 'increasing':
            # è¿‘æœŸ0é ­è™Ÿç¢¼å¢åŠ è¶¨å‹¢ï¼Œæ”¾å¯¬æ¨™æº–
            if pattern['zero_head_count'] > int(recent_avg['zero_head_avg']) + 2:
                return False
        else:
            # è¿‘æœŸ0é ­è™Ÿç¢¼æ¸›å°‘è¶¨å‹¢ï¼Œæ”¶ç·Šæ¨™æº–
            if pattern['zero_head_count'] > int(recent_avg['zero_head_avg']) + 1:
                return False
    
    # é€£è™Ÿè¶¨å‹¢èª¿æ•´
    if 'consecutive_rate_trend' in trends:
        if trends['consecutive_rate_trend'] == 'decreasing':
            # è¿‘æœŸé€£è™Ÿæ¸›å°‘ï¼Œæ›´å‚¾å‘æ–¼é¸æ“‡ç„¡é€£è™Ÿçš„çµ„åˆ
            if pattern['max_consecutive'] > 2:
                return False
    
    # åŸºæœ¬çš„è·¨åº¦æª¢æŸ¥
    if pattern['span'] < 10 or pattern['span'] > 35:
        return False
    
    return True

def get_historical_stats(patterns_df, recent_periods=None, use_weighted=False):
    """
    å¾æ­·å²æ•¸æ“šä¸­æå–çµ±è¨ˆä¿¡æ¯ç”¨æ–¼é¸è™Ÿé©—è­‰
    Args:
        patterns_df: å®Œæ•´çš„æ­·å²æ¨¡å¼æ•¸æ“š
        recent_periods: è¿‘æœŸæœŸæ•¸ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•¸æ“š
        use_weighted: æ˜¯å¦ä½¿ç”¨åŠ æ¬Šåˆ†æï¼ˆè¿‘æœŸæ¬Šé‡æ›´é«˜ï¼‰
    """
    if recent_periods is not None and recent_periods > 0:
        # ä½¿ç”¨è¿‘æœŸæ•¸æ“š
        analysis_df = patterns_df.tail(recent_periods).copy()
        print(f"ğŸ¯ ä½¿ç”¨è¿‘æœŸ {len(analysis_df)} æœŸæ•¸æ“šé€²è¡Œåˆ†æ (å…±{len(patterns_df)}æœŸ)")
    else:
        analysis_df = patterns_df.copy()
        print(f"ğŸ“Š ä½¿ç”¨å…¨éƒ¨ {len(analysis_df)} æœŸæ•¸æ“šé€²è¡Œåˆ†æ")
    
    if use_weighted and len(analysis_df) > 10:
        # åŠ æ¬Šåˆ†æï¼šè¶Šè¿‘æœŸçš„æ•¸æ“šæ¬Šé‡è¶Šé«˜
        weights = create_time_weights(len(analysis_df))
        stats = calculate_weighted_stats(analysis_df, weights)
        print("âš–ï¸ æ¡ç”¨æ™‚é–“åŠ æ¬Šåˆ†æï¼ˆè¿‘æœŸæ¬Šé‡è¼ƒé«˜ï¼‰")
    else:
        # å‚³çµ±çµ±è¨ˆåˆ†æ
        stats = calculate_standard_stats(analysis_df)
        if use_weighted:
            print("ğŸ“‹ æ•¸æ“šé‡ä¸è¶³ï¼Œæ¡ç”¨æ¨™æº–åˆ†æ")
    
    # æ–°å¢è¶¨å‹¢åˆ†æ
    trend_analysis = analyze_recent_trends(patterns_df, analysis_df)
    stats.update(trend_analysis)
    
    return stats

def create_time_weights(n_periods):
    """å‰µå»ºæ™‚é–“æ¬Šé‡ï¼šè¿‘æœŸè³‡æ–™æ¬Šé‡æ›´é«˜"""
    # ä½¿ç”¨æŒ‡æ•¸è¡°æ¸›æ¬Šé‡ï¼Œæœ€æ–°çš„æ¬Šé‡=1ï¼Œæœ€èˆŠçš„æ¬Šé‡ç´„0.1
    weights = np.exp(np.linspace(-2.3, 0, n_periods))  # e^(-2.3) â‰ˆ 0.1
    return weights / weights.sum()  # æ­£è¦åŒ–

def calculate_weighted_stats(df, weights):
    """è¨ˆç®—åŠ æ¬Šçµ±è¨ˆ"""
    stats = {}
    
    # åŠ æ¬Šé »æ¬¡è¨ˆç®—
    for col in ['zero_head_count', 'one_head_count', 'two_head_count', 'three_head_count']:
        weighted_freq = {}
        for i, value in enumerate(df[col]):
            if value in weighted_freq:
                weighted_freq[value] += weights[i]
            else:
                weighted_freq[value] = weights[i]
        
        # æ‰¾å‡ºåŠ æ¬Šé »æ¬¡æœ€é«˜çš„å€¼
        most_common = max(weighted_freq.items(), key=lambda x: x[1])[0]
        stats[f'{col.replace("_count", "")}_most_common'] = most_common
    
    # åŠ æ¬Šé€£è™Ÿæ©Ÿç‡
    consecutive_weight = sum(weights[i] for i, has_consecutive in enumerate(df['has_consecutive']) if has_consecutive)
    stats['consecutive_prob'] = consecutive_weight
    stats['total_periods'] = len(df)
    
    return stats

def calculate_standard_stats(df):
    """è¨ˆç®—æ¨™æº–çµ±è¨ˆ"""
    zero_head_dist = df['zero_head_count'].value_counts()
    one_head_dist = df['one_head_count'].value_counts()
    two_head_dist = df['two_head_count'].value_counts()
    three_head_dist = df['three_head_count'].value_counts()
    
    consecutive_count = df['has_consecutive'].sum()
    total_count = len(df)
    
    return {
        'zero_head_most_common': zero_head_dist.idxmax(),
        'one_head_most_common': one_head_dist.idxmax(),
        'two_head_most_common': two_head_dist.idxmax(),
        'three_head_most_common': three_head_dist.idxmax(),
        'consecutive_prob': consecutive_count / total_count,
        'total_periods': total_count
    }

def analyze_recent_trends(full_df, recent_df):
    """åˆ†æè¿‘æœŸè¶¨å‹¢è®ŠåŒ–"""
    if len(full_df) < 50 or len(recent_df) < 10:
        return {'trend_detected': False}
    
    # æ¯”è¼ƒè¿‘æœŸèˆ‡æ­·å²å¹³å‡çš„å·®ç•°
    full_avg = {
        'zero_head_avg': full_df['zero_head_count'].mean(),
        'one_head_avg': full_df['one_head_count'].mean(),
        'two_head_avg': full_df['two_head_count'].mean(),
        'three_head_avg': full_df['three_head_count'].mean(),
        'consecutive_rate': full_df['has_consecutive'].mean()
    }
    
    recent_avg = {
        'zero_head_avg': recent_df['zero_head_count'].mean(),
        'one_head_avg': recent_df['one_head_count'].mean(),
        'two_head_avg': recent_df['two_head_count'].mean(),
        'three_head_avg': recent_df['three_head_count'].mean(),
        'consecutive_rate': recent_df['has_consecutive'].mean()
    }
    
    # æª¢æ¸¬é¡¯è‘—è¶¨å‹¢è®ŠåŒ–ï¼ˆå·®ç•°è¶…é20%ï¼‰
    trends = {}
    trend_detected = False
    
    for key in full_avg:
        diff_ratio = abs(recent_avg[key] - full_avg[key]) / full_avg[key] if full_avg[key] > 0 else 0
        if diff_ratio > 0.2:  # 20%çš„è®ŠåŒ–é–¾å€¼
            trends[f'{key}_trend'] = 'increasing' if recent_avg[key] > full_avg[key] else 'decreasing'
            trend_detected = True
    
    return {
        'trend_detected': trend_detected,
        'trends': trends,
        'recent_vs_historical': {
            'recent': recent_avg,
            'historical': full_avg
        }
    }

# --------------------------------------------------------
# 5. æ”¹é€²çš„å»ºè­°è™Ÿç¢¼ç”¢ç”Ÿå™¨ ----------------------------------
def suggest_numbers(strategy='smart', n=9, historical_stats=None):
    """
    ç”¢ç”Ÿå»ºè­°è™Ÿç¢¼
    Args:
        strategy: é¸è™Ÿç­–ç•¥ ('random', 'hot', 'cold', 'smart', 'balanced')
        n: è¦é¸æ“‡çš„è™Ÿç¢¼æ•¸é‡ï¼ˆé è¨­9å€‹ï¼‰
        historical_stats: æ­·å²çµ±è¨ˆæ•¸æ“šç”¨æ–¼é©—è­‰
    Returns:
        æ’åºå¾Œçš„nå€‹è™Ÿç¢¼åˆ—è¡¨
    """
    numbers = list(range(1, 40))          # 1-39
    global hot_numbers, cold_numbers

    if strategy == 'random':
        # éš¨æ©Ÿé¸è™Ÿï¼šç›´æ¥é¸æ“‡nå€‹è™Ÿç¢¼
        return sorted(random.sample(numbers, n))

    elif strategy == 'hot':
        # ç†±è™Ÿå„ªå…ˆï¼šå„ªå…ˆé¸æ“‡ç†±è™Ÿï¼Œä¸è¶³æ™‚è£œå……å…¶ä»–è™Ÿç¢¼
        sel = hot_numbers.copy()
        if len(sel) >= n:
            return sorted(random.sample(sel, n))
        else:
            remain = [x for x in numbers if x not in sel]
            sel += random.sample(remain, n - len(sel))
            return sorted(sel)

    elif strategy == 'cold':
        # å†·è™Ÿå„ªå…ˆï¼šå„ªå…ˆé¸æ“‡å†·è™Ÿï¼Œä¸è¶³æ™‚è£œå……å…¶ä»–è™Ÿç¢¼
        sel = cold_numbers.copy()
        if len(sel) >= n:
            return sorted(random.sample(sel, n))
        else:
            remain = [x for x in numbers if x not in sel]
            sel += random.sample(remain, n - len(sel))
            return sorted(sel)
    
    elif strategy == 'smart':
        # æ™ºèƒ½é¸è™Ÿï¼šåš´æ ¼åŸºæ–¼æ­·å²çµ±è¨ˆæ¨¡å¼
        max_attempts = 5000  # å¢åŠ å˜—è©¦æ¬¡æ•¸ä»¥ç¢ºä¿æ‰¾åˆ°ç¬¦åˆæ­·å²æ¨¡å¼çš„çµ„åˆ
        attempts = 0
        
        while attempts < max_attempts:
            # å…ˆç”Ÿæˆä¸€çµ„5å€‹è™Ÿç¢¼ï¼ˆä¸»è¦çµ„åˆï¼‰
            main_group = random.sample(numbers, 5)
            
            if is_pattern_reasonable(main_group, historical_stats):
                # å†å¾å‰©é¤˜è™Ÿç¢¼ä¸­é¸4å€‹
                remaining = [x for x in numbers if x not in main_group]
                additional = random.sample(remaining, 4)
                result = sorted(main_group + additional)
                return result
            attempts += 1
        
        # å¦‚æœç„¡æ³•æ‰¾åˆ°ç†æƒ³çµ„åˆï¼Œé™ä½æ¨™æº–å†è©¦ä¸€æ¬¡
        attempts = 0
        while attempts < 1000:
            main_group = random.sample(numbers, 5)
            if is_pattern_reasonable(main_group):  # ä½¿ç”¨å›ºå®šæ¨™æº–
                remaining = [x for x in numbers if x not in main_group]
                additional = random.sample(remaining, 4)
                result = sorted(main_group + additional)
                return result
            attempts += 1
        
        # æœ€å¾Œé€€å›éš¨æ©Ÿé¸è™Ÿ
        return sorted(random.sample(numbers, n))
    
    elif strategy == 'balanced':
        # å¹³è¡¡ç­–ç•¥ï¼šçµåˆç†±è™Ÿã€å†·è™Ÿï¼Œä¸¦è€ƒæ…®åŸºæœ¬æ¨¡å¼
        max_attempts = 1000
        attempts = 0
        
        while attempts < max_attempts:
            # é¸æ“‡3-4å€‹ç†±è™Ÿ
            hot_count = random.randint(3, 4)
            # é¸æ“‡2-3å€‹å†·è™Ÿ
            cold_count = random.randint(2, 3)
            # å‰©é¤˜éš¨æ©Ÿé¸æ“‡
            random_count = n - hot_count - cold_count
            
            candidates = []
            
            # å¾ç†±è™Ÿä¸­é¸æ“‡
            if len(hot_numbers) >= hot_count:
                candidates.extend(random.sample(hot_numbers, hot_count))
            else:
                candidates.extend(hot_numbers)
                hot_count = len(hot_numbers)
            
            # å¾å†·è™Ÿä¸­é¸æ“‡
            available_cold = [x for x in cold_numbers if x not in candidates]
            actual_cold_count = min(cold_count, len(available_cold))
            if actual_cold_count > 0:
                candidates.extend(random.sample(available_cold, actual_cold_count))
            
            # éš¨æ©Ÿé¸æ“‡å‰©é¤˜çš„
            remaining_numbers = [x for x in numbers if x not in candidates]
            random_count = n - len(candidates)
            if random_count > 0 and len(remaining_numbers) >= random_count:
                candidates.extend(random.sample(remaining_numbers, random_count))
            
            if len(candidates) == n:
                return sorted(candidates)
            attempts += 1
        
        # å¦‚æœç„¡æ³•æ‰¾åˆ°åˆç†çµ„åˆï¼Œé€€å›éš¨æ©Ÿé¸è™Ÿ
        return sorted(random.sample(numbers, n))
    
    elif strategy == 'trend_adaptive':
        # è¶¨å‹¢é©æ‡‰ç­–ç•¥ï¼šæ ¹æ“šè¿‘æœŸè¶¨å‹¢èª¿æ•´é¸è™Ÿ
        if historical_stats is None or not historical_stats.get('trend_detected', False):
            # æ²’æœ‰è¶¨å‹¢æ•¸æ“šï¼Œé€€å›æ™ºèƒ½é¸è™Ÿ
            return suggest_numbers('smart', n, historical_stats)
        
        max_attempts = 3000
        attempts = 0
        trends = historical_stats.get('trends', {})
        
        while attempts < max_attempts:
            main_group = random.sample(numbers, 5)
            
            # æ ¹æ“šè¶¨å‹¢èª¿æ•´é©—è­‰æ¨™æº–
            if is_trend_reasonable(main_group, historical_stats):
                remaining = [x for x in numbers if x not in main_group]
                additional = random.sample(remaining, 4)
                result = sorted(main_group + additional)
                return result
            attempts += 1
        
        # è¶¨å‹¢ç­–ç•¥å¤±æ•—ï¼Œé€€å›æ™ºèƒ½é¸è™Ÿ
        return suggest_numbers('smart', n, historical_stats)

    else:
        raise ValueError(f'unknown strategy {strategy}')

# --------------------------------------------------------
# 6. é æ¸¬è¨˜éŒ„åŠŸèƒ½ -------------------------------------------
def check_daily_analysis_exists(log_file="prediction_log.xlsx"):
    """
    æª¢æŸ¥ä»Šæ—¥æ˜¯å¦å·²æœ‰åˆ†æè¨˜éŒ„
    Returns:
        bool: Trueè¡¨ç¤ºä»Šæ—¥å·²æœ‰è¨˜éŒ„ï¼ŒFalseè¡¨ç¤ºä»Šæ—¥å°šç„¡è¨˜éŒ„
    """
    current_time = datetime.now()
    date_str = current_time.strftime("%Y-%m-%d")
    
    log_path = Path(log_file)
    if not log_path.exists():
        return False
    
    try:
        existing_df = pd.read_excel(log_file, engine='openpyxl')
        today_records = existing_df[existing_df['æ—¥æœŸ'] == date_str]
        
        if len(today_records) > 0:
            print(f"ğŸ“… æª¢æ¸¬åˆ°ä»Šæ—¥({date_str})å·²æœ‰åˆ†æè¨˜éŒ„")
            # é¡¯ç¤ºå·²æœ‰è¨˜éŒ„çš„æ‘˜è¦
            latest_record = today_records.iloc[-1]
            print(f"   è¨˜éŒ„æ™‚é–“: {latest_record.get('æ™‚é–“', 'N/A')}")
            if 'æ™ºèƒ½é¸è™Ÿ' in latest_record and pd.notna(latest_record['æ™ºèƒ½é¸è™Ÿ']):
                print(f"   æ™ºèƒ½é¸è™Ÿ: {latest_record['æ™ºèƒ½é¸è™Ÿ']}")
            if 'å¹³è¡¡ç­–ç•¥' in latest_record and pd.notna(latest_record['å¹³è¡¡ç­–ç•¥']):
                print(f"   å¹³è¡¡ç­–ç•¥: {latest_record['å¹³è¡¡ç­–ç•¥']}")
            return True
        else:
            return False
    except Exception as e:
        print(f"âš ï¸ æª¢æŸ¥ä»Šæ—¥è¨˜éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False

def log_predictions_to_excel(predictions, log_file="prediction_log.xlsx", allow_overwrite=False):
    """
    å°‡é æ¸¬çµæœè¨˜éŒ„åˆ°Excelæª”æ¡ˆ
    Args:
        predictions: åŒ…å«å„ç­–ç•¥é æ¸¬çµæœçš„å­—å…¸
        log_file: è¨˜éŒ„æª”æ¡ˆåç¨±
        allow_overwrite: æ˜¯å¦å…è¨±è¦†è“‹ä»Šæ—¥å·²æœ‰è¨˜éŒ„ï¼ˆé è¨­Falseä¸è¦†è“‹ï¼‰
    """
    current_time = datetime.now()
    date_str = current_time.strftime("%Y-%m-%d")
    time_str = current_time.strftime("%H:%M:%S")
    
    # æª¢æŸ¥ä»Šæ—¥æ˜¯å¦å·²æœ‰è¨˜éŒ„
    if not allow_overwrite and check_daily_analysis_exists(log_file):
        print(f"âš ï¸ ä»Šæ—¥({date_str})å·²æœ‰åˆ†æè¨˜éŒ„ï¼Œè·³éè¨˜éŒ„ä»¥é¿å…è¦†è“‹")
        print(f"   å¦‚éœ€å¼·åˆ¶è¦†è“‹ï¼Œè«‹æ‰‹å‹•åˆªé™¤ä»Šæ—¥è¨˜éŒ„æˆ–ä½¿ç”¨ allow_overwrite=True åƒæ•¸")
        return False
    
    # æº–å‚™è¨˜éŒ„æ•¸æ“š
    log_data = {
        'æ—¥æœŸ': date_str,
        'æ™‚é–“': time_str,
        'æ™ºèƒ½é¸è™Ÿ': str(predictions.get('smart', [])),
        'å¹³è¡¡ç­–ç•¥': str(predictions.get('balanced', [])),
        'éš¨æ©Ÿé¸è™Ÿ': str(predictions.get('random', [])),
        'ç†±è™Ÿå„ªå…ˆ': str(predictions.get('hot', [])),
        'å†·è™Ÿå„ªå…ˆ': str(predictions.get('cold', [])),
        'æ™ºèƒ½é¸è™Ÿ_å‰5è™Ÿ': str(predictions.get('smart', [])[:5]) if predictions.get('smart') else '',
        'å¹³è¡¡ç­–ç•¥_å‰5è™Ÿ': str(predictions.get('balanced', [])[:5]) if predictions.get('balanced') else '',
        'é©—è­‰çµæœ': '',  # ç•™ç©ºï¼Œç­‰å¾…æœªä¾†é–‹ççµæœ
        'ä¸­çè™Ÿç¢¼æ•¸': '',  # ç•™ç©ºï¼Œç­‰å¾…é©—è­‰
        'å‚™è¨»': f"åŸºæ–¼æ­·å²çµ±è¨ˆæ¨¡å¼ç”Ÿæˆ"
    }
    
    # å¦‚æœæœ‰è¶¨å‹¢é©æ‡‰ç­–ç•¥ï¼Œä¹Ÿè¨˜éŒ„
    if 'trend_adaptive' in predictions:
        log_data['è¶¨å‹¢é©æ‡‰'] = str(predictions['trend_adaptive'])
        log_data['è¶¨å‹¢é©æ‡‰_å‰5è™Ÿ'] = str(predictions['trend_adaptive'][:5])
    
    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    log_path = Path(log_file)
    overwrite_occurred = False
    
    if log_path.exists():
        # è®€å–ç¾æœ‰æ•¸æ“š
        try:
            existing_df = pd.read_excel(log_file, engine='openpyxl')
            
            # æª¢æŸ¥ä»Šå¤©æ˜¯å¦å·²æœ‰è¨˜éŒ„
            today_records = existing_df[existing_df['æ—¥æœŸ'] == date_str]
            
            if len(today_records) > 0 and allow_overwrite:
                # ä»Šå¤©å·²æœ‰è¨˜éŒ„ä¸”å…è¨±è¦†è“‹ï¼Œè¦†è“‹æœ€æ–°çš„ä¸€ç­†
                latest_today_index = today_records.index[-1]  # å–å¾—ä»Šå¤©æœ€å¾Œä¸€ç­†çš„ç´¢å¼•
                
                # ä¿ç•™å·²é©—è­‰çš„çµæœï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
                old_record = existing_df.loc[latest_today_index]
                if pd.notna(old_record.get('é©—è­‰çµæœ', '')) and old_record.get('é©—è­‰çµæœ', '') != '':
                    log_data['é©—è­‰çµæœ'] = old_record['é©—è­‰çµæœ']
                    log_data['ä¸­çè™Ÿç¢¼æ•¸'] = old_record['ä¸­çè™Ÿç¢¼æ•¸']
                    print(f"ğŸ”„ è¦†è“‹ä»Šæ—¥è¨˜éŒ„ï¼Œä¿ç•™å·²é©—è­‰çµæœ")
                else:
                    print(f"ğŸ”„ è¦†è“‹ä»Šæ—¥è¨˜éŒ„")
                
                # è¦†è“‹è©²è¨˜éŒ„
                for key, value in log_data.items():
                    if key in existing_df.columns:
                        existing_df.loc[latest_today_index, key] = value
                    else:
                        # å¦‚æœæ˜¯æ–°æ¬„ä½ï¼ˆå¦‚è¶¨å‹¢é©æ‡‰ï¼‰ï¼Œéœ€è¦å…ˆæ·»åŠ æ¬„ä½
                        existing_df[key] = ''
                        existing_df.loc[latest_today_index, key] = value
                
                combined_df = existing_df
                overwrite_occurred = True
            else:
                # ä»Šå¤©æ²’æœ‰è¨˜éŒ„æˆ–ä¸å…è¨±è¦†è“‹ï¼Œæ–°å¢
                new_df = pd.DataFrame([log_data])
                
                # æª¢æŸ¥æ˜¯å¦æœ‰æ–°æ¬„ä½éœ€è¦æ·»åŠ åˆ°ç¾æœ‰æ•¸æ“š
                for col in new_df.columns:
                    if col not in existing_df.columns:
                        existing_df[col] = ''
                
                combined_df = pd.concat([existing_df, new_df], ignore_index=True)
                
        except Exception as e:
            print(f"è®€å–ç¾æœ‰è¨˜éŒ„æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            # å¦‚æœè®€å–å¤±æ•—ï¼Œå‰µå»ºæ–°çš„DataFrame
            combined_df = pd.DataFrame([log_data])
    else:
        # å‰µå»ºæ–°çš„DataFrame
        combined_df = pd.DataFrame([log_data])
    
    # å¯«å…¥Excel
    try:
        combined_df.to_excel(log_file, index=False, engine='openpyxl')
        
        if overwrite_occurred:
            print(f"\nğŸ”„ é æ¸¬è¨˜éŒ„å·²è¦†è“‹æ›´æ–°: {log_file}")
            print(f"   æ›´æ–°æ™‚é–“: {date_str} {time_str}")
            print(f"   âš ï¸  åŒä¸€å¤©çš„èˆŠè¨˜éŒ„å·²è¢«æ›¿æ›")
        else:
            print(f"\nâœ… é æ¸¬è¨˜éŒ„å·²ä¿å­˜åˆ°: {log_file}")
            print(f"   è¨˜éŒ„æ™‚é–“: {date_str} {time_str}")
        
        return True
            
    except Exception as e:
        print(f"âŒ ä¿å­˜é æ¸¬è¨˜éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False

def analyze_prediction_accuracy(log_file="prediction_log.xlsx", actual_results_file="lottery_hist.xlsx"):
    """
    åˆ†æé æ¸¬æº–ç¢ºåº¦ï¼ˆæœªä¾†åŠŸèƒ½ï¼Œç•¶æœ‰è¶³å¤ æ­·å²é æ¸¬è¨˜éŒ„æ™‚ä½¿ç”¨ï¼‰
    """
    if not Path(log_file).exists():
        print("å°šç„¡é æ¸¬è¨˜éŒ„æª”æ¡ˆ")
        return
    
    try:
        predictions_df = pd.read_excel(log_file, engine='openpyxl')
        print(f"\nğŸ“Š é æ¸¬è¨˜éŒ„çµ±è¨ˆ:")
        print(f"   ç¸½é æ¸¬æ¬¡æ•¸: {len(predictions_df)}")
        print(f"   æœ€æ—©é æ¸¬: {predictions_df['æ—¥æœŸ'].min()}")
        print(f"   æœ€æ–°é æ¸¬: {predictions_df['æ—¥æœŸ'].max()}")
        print(f"   âš ï¸  é©—è­‰åŠŸèƒ½å°‡åœ¨ç´¯ç©è¶³å¤ æ•¸æ“šå¾Œå¯¦ä½œ")
    except Exception as e:
        print(f"è®€å–é æ¸¬è¨˜éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

# --------------------------------------------------------
# 5. æœŸæœ›ç²åˆ©è¨ˆç®— -------------------------------------------
def expected_profit(n: int = 9,
                    cost_per_ticket: int = 1400,
                    payout_per_hit: int = 11000):
    prob_k = n * 5 / 39
    exp_payout = payout_per_hit * prob_k
    cost = cost_per_ticket * n
    return exp_payout - cost

# --------------------------------------------------------
# 6. ä¸»ç¨‹å¼ ------------------------------------------------------
if __name__ == "__main__":

    # è·¯å¾‘ï¼ˆè«‹è‡ªè¡Œæ”¹æˆå¯¦éš›æª”åï¼‰
    excel_file = "lottery_hist.xlsx"

    if not Path(excel_file).exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æª”æ¡ˆ: {excel_file}")

    # ---------- è‡ªå‹•é©—è­‰ä¹‹å‰çš„é æ¸¬ ----------
    if VERIFICATION_AVAILABLE:
        auto_verify_on_startup("prediction_log.xlsx", excel_file)
    
    print("\n" + "="*60)
    print("ğŸ² 539å½©ç¥¨åˆ†æç³»çµ±")
    print("="*60)
    
    # ---------- æª¢æŸ¥ä»Šæ—¥æ˜¯å¦å·²æœ‰åˆ†æè¨˜éŒ„ ----------
    if check_daily_analysis_exists("prediction_log.xlsx"):
        print("\nâœ… ä»Šæ—¥åˆ†æå·²å®Œæˆï¼Œç¨‹å¼çµæŸ")
        print("ğŸ’¡ å¦‚éœ€é‡æ–°åˆ†æï¼Œè«‹æ‰‹å‹•åˆªé™¤ä»Šæ—¥è¨˜éŒ„æˆ–ä¿®æ”¹ç¨‹å¼ç¢¼")
        exit(0)

    df = load_lottery_excel(excel_file)

    # ---------- åŸºæœ¬çµ±è¨ˆ ----------
    freq_series = compute_num_frequency(df)
    hot_numbers, cold_numbers = get_hot_cold_numbers(freq_series, top_n=6, bottom_n=6)

    print("\n=== æœ€é«˜é »æ¬¡è™Ÿç¢¼ï¼ˆç†±è™Ÿï¼‰ ===")
    print(hot_numbers)
    print("\n=== æœ€ä½é »æ¬¡è™Ÿç¢¼ï¼ˆå†·è™Ÿï¼‰ ===")
    print(cold_numbers)

    # ---------- æ¨¡å¼åˆ†æ ----------
    print("\n" + "="*50)
    print("é–‹å§‹é€²è¡Œæ­·å²é–‹çæ¨¡å¼åˆ†æ...")
    patterns_df = pattern_statistics(df)
    
    # æå–æ­·å²çµ±è¨ˆæ•¸æ“šç”¨æ–¼æ™ºèƒ½é¸è™Ÿ
    print(f"\nğŸ”¬ çµ±è¨ˆåˆ†ææ¨¡å¼é¸æ“‡:")
    
    # æ ¹æ“šæ•¸æ“šé‡æ±ºå®šåˆ†æç­–ç•¥
    total_periods = len(patterns_df)
    
    if total_periods >= 200:
        # æ•¸æ“šå……è¶³ï¼Œæä¾›å¤šç¨®åˆ†æé¸é …
        print(f"ğŸ“Š æª¢æ¸¬åˆ°å……è¶³çš„æ­·å²æ•¸æ“š ({total_periods} æœŸ)")
        
        # 1. å…¨éƒ¨æ•¸æ“šåˆ†æ
        historical_stats_full = get_historical_stats(patterns_df)
        
        # 2. è¿‘æœŸæ•¸æ“šåˆ†æï¼ˆæœ€è¿‘100æœŸï¼‰
        recent_periods = min(100, total_periods // 2)
        historical_stats_recent = get_historical_stats(patterns_df, recent_periods=recent_periods)
        
        # 3. åŠ æ¬Šåˆ†æï¼ˆè¿‘æœŸæ¬Šé‡æ›´é«˜ï¼‰
        historical_stats_weighted = get_historical_stats(patterns_df, recent_periods=recent_periods, use_weighted=True)
        
        # é¸æ“‡ä¸»è¦åˆ†ææ¨¡å¼
        if historical_stats_recent.get('trend_detected', False):
            print(f"ğŸ“ˆ æª¢æ¸¬åˆ°è¿‘æœŸè¶¨å‹¢è®ŠåŒ–ï¼Œæ¡ç”¨è¿‘æœŸåŠ æ¬Šåˆ†æ")
            historical_stats = historical_stats_weighted
            analysis_mode = "è¿‘æœŸåŠ æ¬Š"
        else:
            print(f"ğŸ“‹ è¿‘æœŸæ¨¡å¼ç©©å®šï¼Œæ¡ç”¨è¿‘æœŸæ•¸æ“šåˆ†æ")
            historical_stats = historical_stats_recent
            analysis_mode = "è¿‘æœŸæ•¸æ“š"
    
    elif total_periods >= 50:
        # ä¸­ç­‰æ•¸æ“šé‡ï¼Œä½¿ç”¨è¿‘æœŸåˆ†æ
        recent_periods = min(30, total_periods // 2)
        historical_stats = get_historical_stats(patterns_df, recent_periods=recent_periods)
        analysis_mode = "è¿‘æœŸæ•¸æ“š"
        print(f"ğŸ“Š ä½¿ç”¨è¿‘æœŸ {recent_periods} æœŸæ•¸æ“šé€²è¡Œåˆ†æ")
    
    else:
        # æ•¸æ“šä¸è¶³ï¼Œä½¿ç”¨å…¨éƒ¨æ•¸æ“š
        historical_stats = get_historical_stats(patterns_df)
        analysis_mode = "å…¨éƒ¨æ•¸æ“š"
        print(f"ğŸ“Š æ•¸æ“šé‡è¼ƒå°‘ï¼Œä½¿ç”¨å…¨éƒ¨ {total_periods} æœŸæ•¸æ“š")
    
    # é¡¯ç¤ºçµ±è¨ˆåŸºæº–
    print(f"\nğŸ¯ çµ±è¨ˆåŸºæº– ({analysis_mode}):")
    print(f"   æœ€å¸¸è¦‹çš„0é ­è™Ÿç¢¼æ•¸é‡: {historical_stats['zero_head_most_common']}å€‹")
    print(f"   æœ€å¸¸è¦‹çš„1é ­è™Ÿç¢¼æ•¸é‡: {historical_stats['one_head_most_common']}å€‹")
    print(f"   æœ€å¸¸è¦‹çš„2é ­è™Ÿç¢¼æ•¸é‡: {historical_stats['two_head_most_common']}å€‹")
    print(f"   æœ€å¸¸è¦‹çš„3é ­è™Ÿç¢¼æ•¸é‡: {historical_stats['three_head_most_common']}å€‹")
    print(f"   é€£è™Ÿå‡ºç¾æ©Ÿç‡: {historical_stats['consecutive_prob']:.2%}")
    
    # é¡¯ç¤ºè¶¨å‹¢åˆ†æçµæœ
    if historical_stats.get('trend_detected', False):
        print(f"\nğŸ“ˆ è¿‘æœŸè¶¨å‹¢åˆ†æ:")
        trends = historical_stats.get('trends', {})
        for trend_key, trend_direction in trends.items():
            trend_name = trend_key.replace('_trend', '').replace('_', ' ')
            direction_text = "å¢åŠ " if trend_direction == 'increasing' else "æ¸›å°‘"
            print(f"   {trend_name}: {direction_text}è¶¨å‹¢")
        
        recent_data = historical_stats.get('recent_vs_historical', {})
        if recent_data:
            print(f"\nğŸ“Š è¿‘æœŸ vs æ­·å²æ¯”è¼ƒ:")
            for key in ['zero_head_avg', 'consecutive_rate']:
                if key in recent_data['recent']:
                    recent_val = recent_data['recent'][key]
                    hist_val = recent_data['historical'][key]
                    change = ((recent_val - hist_val) / hist_val * 100) if hist_val > 0 else 0
                    key_name = key.replace('_avg', '').replace('_', ' ')
                    print(f"   {key_name}: {recent_val:.2f} vs {hist_val:.2f} ({change:+.1f}%)")

    # ------------- å¯è¦–åŒ–å·²ç§»é™¤ ----------
    # plot_frequency(freq_series)  # å·²ç§»é™¤å¯è¦–åŒ–åŠŸèƒ½

    # ------------- åŸºæ–¼æ­·å²çµ±è¨ˆçš„å»ºè­°è™Ÿç¢¼ ----------
    print("\n" + "="*50)
    print("æ ¹æ“šæ­·å²æ¨¡å¼ç”Ÿæˆ9å€‹å»ºè­°è™Ÿç¢¼...")
    
    # ç”Ÿæˆå„ç­–ç•¥çš„å»ºè­°è™Ÿç¢¼
    print("\n===== æ™ºèƒ½é¸è™Ÿ (9å€‹è™Ÿç¢¼) =====")
    smart_numbers = suggest_numbers('smart', n=9, historical_stats=historical_stats)
    print(f"å»ºè­°è™Ÿç¢¼: {smart_numbers}")
    # åˆ†æå‰5å€‹è™Ÿç¢¼çš„æ¨¡å¼ä½œç‚ºåƒè€ƒ
    first_five = smart_numbers[:5]
    pattern = analyze_single_draw_patterns(first_five)
    print(f"å‰5è™Ÿæ¨¡å¼: 0é ­:{pattern['zero_head_count']}å€‹, 1é ­:{pattern['one_head_count']}å€‹, 2é ­:{pattern['two_head_count']}å€‹, 3é ­:{pattern['three_head_count']}å€‹")
    print(f"          é€£è™Ÿ:{pattern['max_consecutive']}å€‹, å¥‡æ•¸:{pattern['odd_count']}å€‹, é›™æ•¸:{pattern['even_count']}å€‹, è·¨åº¦:{pattern['span']}")
    
    print("\n===== å¹³è¡¡ç­–ç•¥ (9å€‹è™Ÿç¢¼) =====")
    balanced_numbers = suggest_numbers('balanced', n=9, historical_stats=historical_stats)
    print(f"å»ºè­°è™Ÿç¢¼: {balanced_numbers}")
    first_five = balanced_numbers[:5]
    pattern = analyze_single_draw_patterns(first_five)
    print(f"å‰5è™Ÿæ¨¡å¼: 0é ­:{pattern['zero_head_count']}å€‹, 1é ­:{pattern['one_head_count']}å€‹, 2é ­:{pattern['two_head_count']}å€‹, 3é ­:{pattern['three_head_count']}å€‹")
    print(f"          é€£è™Ÿ:{pattern['max_consecutive']}å€‹, å¥‡æ•¸:{pattern['odd_count']}å€‹, é›™æ•¸:{pattern['even_count']}å€‹, è·¨åº¦:{pattern['span']}")

    print("\n===== éš¨æ©Ÿé¸è™Ÿ (9å€‹è™Ÿç¢¼) =====")
    random_numbers = suggest_numbers('random', n=9)
    print(f"å»ºè­°è™Ÿç¢¼: {random_numbers}")
    first_five = random_numbers[:5]
    pattern = analyze_single_draw_patterns(first_five)
    print(f"å‰5è™Ÿæ¨¡å¼: 0é ­:{pattern['zero_head_count']}å€‹, 1é ­:{pattern['one_head_count']}å€‹, 2é ­:{pattern['two_head_count']}å€‹, 3é ­:{pattern['three_head_count']}å€‹")
    print(f"          é€£è™Ÿ:{pattern['max_consecutive']}å€‹, å¥‡æ•¸:{pattern['odd_count']}å€‹, é›™æ•¸:{pattern['even_count']}å€‹, è·¨åº¦:{pattern['span']}")

    print("\n===== ç†±è™Ÿå„ªå…ˆ (9å€‹è™Ÿç¢¼) =====")
    hot_numbers_selected = suggest_numbers('hot', n=9)
    print(f"å»ºè­°è™Ÿç¢¼: {hot_numbers_selected}")
    first_five = hot_numbers_selected[:5]
    pattern = analyze_single_draw_patterns(first_five)
    print(f"å‰5è™Ÿæ¨¡å¼: 0é ­:{pattern['zero_head_count']}å€‹, 1é ­:{pattern['one_head_count']}å€‹, 2é ­:{pattern['two_head_count']}å€‹, 3é ­:{pattern['three_head_count']}å€‹")
    print(f"          é€£è™Ÿ:{pattern['max_consecutive']}å€‹, å¥‡æ•¸:{pattern['odd_count']}å€‹, é›™æ•¸:{pattern['even_count']}å€‹, è·¨åº¦:{pattern['span']}")

    print("\n===== å†·è™Ÿå„ªå…ˆ (9å€‹è™Ÿç¢¼) =====")
    cold_numbers_selected = suggest_numbers('cold', n=9)
    print(f"å»ºè­°è™Ÿç¢¼: {cold_numbers_selected}")
    first_five = cold_numbers_selected[:5]
    pattern = analyze_single_draw_patterns(first_five)
    print(f"å‰5è™Ÿæ¨¡å¼: 0é ­:{pattern['zero_head_count']}å€‹, 1é ­:{pattern['one_head_count']}å€‹, 2é ­:{pattern['two_head_count']}å€‹, 3é ­:{pattern['three_head_count']}å€‹")
    print(f"          é€£è™Ÿ:{pattern['max_consecutive']}å€‹, å¥‡æ•¸:{pattern['odd_count']}å€‹, é›™æ•¸:{pattern['even_count']}å€‹, è·¨åº¦:{pattern['span']}")

    # è¶¨å‹¢é©æ‡‰ç­–ç•¥ï¼ˆå¦‚æœæª¢æ¸¬åˆ°è¶¨å‹¢ï¼‰
    trend_numbers = None
    if historical_stats.get('trend_detected', False):
        print("\n===== è¶¨å‹¢é©æ‡‰ (9å€‹è™Ÿç¢¼) =====")
        trend_numbers = suggest_numbers('trend_adaptive', n=9, historical_stats=historical_stats)
        print(f"å»ºè­°è™Ÿç¢¼: {trend_numbers}")
        first_five = trend_numbers[:5]
        pattern = analyze_single_draw_patterns(first_five)
        print(f"å‰5è™Ÿæ¨¡å¼: 0é ­:{pattern['zero_head_count']}å€‹, 1é ­:{pattern['one_head_count']}å€‹, 2é ­:{pattern['two_head_count']}å€‹, 3é ­:{pattern['three_head_count']}å€‹")
        print(f"          é€£è™Ÿ:{pattern['max_consecutive']}å€‹, å¥‡æ•¸:{pattern['odd_count']}å€‹, é›™æ•¸:{pattern['even_count']}å€‹, è·¨åº¦:{pattern['span']}")
        print(f"ğŸ’¡ æ­¤ç­–ç•¥æ ¹æ“šæª¢æ¸¬åˆ°çš„è¿‘æœŸè¶¨å‹¢é€²è¡Œé¸è™Ÿèª¿æ•´")
    else:
        print("\nğŸ“‹ æœªæª¢æ¸¬åˆ°é¡¯è‘—è¶¨å‹¢ï¼Œè·³éè¶¨å‹¢é©æ‡‰ç­–ç•¥")
    
    # ------------- è¨˜éŒ„é æ¸¬çµæœ ----------
    predictions = {
        'smart': smart_numbers,
        'balanced': balanced_numbers,
        'random': random_numbers,
        'hot': hot_numbers_selected,
        'cold': cold_numbers_selected
    }
    
    # å¦‚æœæœ‰è¶¨å‹¢ç­–ç•¥çµæœï¼Œä¹Ÿè¨˜éŒ„
    if trend_numbers is not None:
        predictions['trend_adaptive'] = trend_numbers
    
    print("\n" + "="*50)
    print("æ­£åœ¨è¨˜éŒ„é æ¸¬çµæœ...")
    log_predictions_to_excel(predictions, "prediction_log.xlsx")
    
    # é¡¯ç¤ºé æ¸¬è¨˜éŒ„çµ±è¨ˆ
    analyze_prediction_accuracy("prediction_log.xlsx")

    # ------------- æœŸæœ›ç²åˆ© ----------
    exp = expected_profit()
    print(f"\næœŸæœ›ç²åˆ©ï¼ˆå›ºå®š 9 å€‹è™Ÿç¢¼ï¼‰:  {exp:.2f} å°å¹£")

    # ------------- åˆ†æç¸½çµ ----------
    print("\n" + "="*50)
    print("ğŸ“Š åˆ†æç¸½çµ:")
    print("1. æ™ºèƒ½é¸è™Ÿå’Œå¹³è¡¡ç­–ç•¥æœƒè‡ªå‹•é¿å…æ­·å²ä¸Šå¾ˆå°‘å‡ºç¾çš„æ¨¡å¼")
    print("2. è«‹åƒè€ƒä¸Šæ–¹çš„æ¨¡å¼çµ±è¨ˆä¾†äº†è§£åˆç†çš„è™Ÿç¢¼çµ„åˆç‰¹å¾µ")
    print("3. å»ºè­°è§€å¯Ÿä¸åŒç­–ç•¥ç”Ÿæˆçš„è™Ÿç¢¼æ¨¡å¼å·®ç•°")
    
    # ------------- è­¦ç¤º ----------
    print("\nâš ï¸ é‡è¦æé†’:")
    print("â€¢ ä»»ä½•é¸è™Ÿç­–ç•¥éƒ½ç„¡æ³•æ”¹è®Šä¸­çæ©Ÿç‡")
    print("â€¢ æ­·å²æ¨¡å¼åˆ†æåƒ…ä¾›åƒè€ƒï¼Œä¸ä»£è¡¨æœªä¾†è¶¨å‹¢")
    print("â€¢ ç†æ€§æŠ•æ³¨ï¼Œé‡åŠ›è€Œç‚º")
